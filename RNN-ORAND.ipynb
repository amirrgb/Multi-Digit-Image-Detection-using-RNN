{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go for RNN based on https://github.com/AbhishekAnand18/ImageTextRecognition/blob/master/ImageTextRecognition_Code.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os\n",
    "from skimage.filters import threshold_local\n",
    "import keras\n",
    "import random\n",
    "from keras import backend as K\n",
    "from skimage.morphology import skeletonize\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11719\n",
      "11719\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"./ORAND-CAR-2014/\"\n",
    "CAR_A_test_images_path = dir_path + 'CAR-A/a_test_images/'\n",
    "CAR_A_train_images_path = dir_path + 'CAR-A/a_train_images/'\n",
    "CAR_B_test_images_path = dir_path + 'CAR-B/b_test_images/'\n",
    "CAR_B_train_images_path = dir_path + 'CAR-B/b_train_images/'\n",
    "\n",
    "def load_original_images():\n",
    "    images_path = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_path = os.path.join(root, file)\n",
    "                images_path.append(image_path.replace('\\\\','/'))\n",
    "    return images_path\n",
    "    \n",
    "images_path = load_original_images()\n",
    "\n",
    "def calculate_digit_count(label):\n",
    "    return len(label)\n",
    "\n",
    "def get_labels(image_dir,text_path):\n",
    "  with open(text_path,'r') as f :\n",
    "    lines = f.readlines()\n",
    "  listt = []\n",
    "  for line in lines :\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    listt.append([image_dir + parts[0],parts[1]])\n",
    "  DF = pd.DataFrame(listt)\n",
    "  DF = DF.rename(columns={0: 'image_path', 1: 'label'})\n",
    "  return DF\n",
    "\n",
    "def get_all_labels():\n",
    "  CAR_A_test_text = dir_path + 'CAR-A/a_test_gt.txt'\n",
    "  CAR_A_train_text = dir_path + 'CAR-A/a_train_gt.txt'\n",
    "  CAR_B_test_text = dir_path + 'CAR-B/b_test_gt.txt'\n",
    "  CAR_B_train_text = dir_path + 'CAR-B/b_train_gt.txt'\n",
    "  a_test_label_df = get_labels(CAR_A_test_images_path,CAR_A_test_text)\n",
    "  a_train_label_df = get_labels(CAR_A_train_images_path,CAR_A_train_text)\n",
    "  b_test_label_df = get_labels(CAR_B_test_images_path,CAR_B_test_text)\n",
    "  b_train_label_df = get_labels(CAR_B_train_images_path,CAR_B_train_text)\n",
    "  all_labels = pd.concat([a_test_label_df , a_train_label_df , b_test_label_df , b_train_label_df],ignore_index=True)#.reset_index()\n",
    "  return all_labels\n",
    "\n",
    "def reset_data():\n",
    "  all_labels = get_all_labels()\n",
    "  print(len(all_labels))\n",
    "  print(len(all_labels['image_path'].unique())) # no dupicate image name\n",
    "  all_labels[\"actual_digit_count\"] = all_labels[\"label\"].astype(str).apply(calculate_digit_count)\n",
    "  return all_labels\n",
    "\n",
    "all_labels = reset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_dataFrame(all_labels,column):\n",
    "  height_list = []\n",
    "  width_list = []\n",
    "  for image_path in all_labels[column]:\n",
    "    height = Image.open(image_path).height\n",
    "    width = Image.open(image_path).width\n",
    "    \n",
    "    height_list.append(height)\n",
    "    width_list.append(width)\n",
    "\n",
    "  sizes_df = pd.DataFrame({'width':width_list,'height':height_list})\n",
    "  return sizes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing images functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_image(image_label_df,from_path):\n",
    "    random.seed(42)\n",
    "    indices = list(range(len(image_label_df)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for idx in indices[:10]+[21,22]:\n",
    "        target_image = image_label_df[from_path][idx]\n",
    "        print(\"label:\", image_label_df['label'][idx])\n",
    "        print(\"target:\", target_image)\n",
    "        image = mpimg.imread(target_image)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(df, from_path,new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        img = cv2.imread(img_path)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, gray_img)\n",
    "        \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        denoised_image = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, denoised_image)\n",
    "        \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_images_fastnlmeans(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        denoised_img = cv2.fastNlMeansDenoising(gray_img, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, denoised_img)\n",
    "\n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_threshold_images(df, from_path, new_dir_path, block_size, offset):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        binary_img = threshold_local(gray_img, block_size, offset=offset, method='gaussian', mode='reflect', cval=0)\n",
    "        binary_img = (binary_img * 255).astype(np.uint8)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, binary_img)\n",
    "\n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        im = Image.open(img_path)\n",
    "        im = np.array(im)\n",
    "        _, binary_im = cv2.threshold(im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        binary_im = Image.fromarray(binary_im)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        binary_im.save(save_path)\n",
    "\n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        bin_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        inverted_img = cv2.bitwise_not(bin_img)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, inverted_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        inverted_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        dilated_img = cv2.dilate(inverted_img, kernel, iterations=1)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, dilated_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        inverted_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        eroded_img = cv2.erode(inverted_img, kernel, iterations=1)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, eroded_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        inverted_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        _, binary_img = cv2.threshold(inverted_img, 128, 255, cv2.THRESH_BINARY)\n",
    "        skeleton = skeletonize(binary_img / 255)\n",
    "        thin_img = (skeleton * 255).astype(np.uint8)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, thin_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_images(df, from_path, new_dir_path):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        inverted_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        _, binary_img = cv2.threshold(inverted_img, 128, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            contour_img = np.zeros_like(binary_img)\n",
    "            cv2.drawContours(contour_img, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "            binary_img = cv2.bitwise_xor(binary_img, contour_img)\n",
    "        \n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, binary_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_edges_canny(df, from_path, new_dir_path, low_threshold, high_threshold):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        edges = cv2.Canny(gray_img, low_threshold, high_threshold)\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, edges)\n",
    "        \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize to specific height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_with_height(df,from_path, new_dir_path, target_height):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        height, width = img.shape\n",
    "        target_width = int(width * (target_height / height))\n",
    "        resized_img = cv2.resize(img, (target_width, target_height))\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, resized_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize to specific height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(df,from_path, new_dir_path, target_height,target_width):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        resized_img = cv2.resize(img, (target_width, target_height))\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, resized_img)\n",
    "    \n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_white_padding(df,from_path,new_dir_path, padding_width):\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, img_path in enumerate(df[from_path]):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        height, width = img.shape\n",
    "        new_width = width + 2 * padding_width\n",
    "        padded_img = 255 * np.ones((height, new_width), dtype=np.uint8)\n",
    "        start_col = padding_width\n",
    "        padded_img[:, start_col:start_col + width] = img\n",
    "        save_path = os.path.join(new_dir_path, f\"{i:05d}.png\")\n",
    "        cv2.imwrite(save_path, padded_img)\n",
    "\n",
    "    df[new_dir_path] = [os.path.join(new_dir_path, f\"{i:05d}.png\") for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11719\n",
      "11719\n"
     ]
    }
   ],
   "source": [
    "all_labels = reset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_image(all_labels,'image_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray scale\n",
    "all_labels = convert_to_grayscale(all_labels,'image_path','./grayscaled/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to specific height\n",
    "# target_height = 28\n",
    "# all_labels = resize_images_with_height(all_labels,'./grayscaled/', './resized/', target_height)\n",
    "all_labels = resize_images(all_labels,'./grayscaled/', './resized/', target_height=54,target_width=174)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding from right and left\n",
    "\n",
    "# padding_width = 28\n",
    "# all_labels = add_white_padding(all_labels, './resized/','./padded/', padding_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising\n",
    "\n",
    "# all_labels = denoise_images(all_labels, 'image_path', './denoised/')\n",
    "# all_labels = denoise_images_fastnlmeans(all_labels, './grayscaled/', './denoised_fastnlmeans/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizing and inverting\n",
    "\n",
    "# block_size = 9\n",
    "# offset = 0\n",
    "# all_labels = local_threshold_images(all_labels, './edges/', './local_thresholded/', block_size, offset)\n",
    "# all_labels = binarize_images(all_labels, './denoised_fastnlmeans/', './binarized/')\n",
    "# all_labels = invert_images(all_labels, './denoised_fastnlmeans/', './inverted/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thinning and delating\n",
    "\n",
    "# all_labels = dilate_images(all_labels, './edges/', './dilated/')\n",
    "# all_labels = erode_images(all_labels, './inverted/', './eroded/')\n",
    "# all_labels = thin_images(all_labels, './inverted/', './thinned/')\n",
    "# all_labels = thin_images(all_labels, './inverted/', './c_thinned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge detecting with canny \n",
    "\n",
    "# low_threshold = 50\n",
    "# high_threshold = 150\n",
    "# all_labels = detect_edges_canny(all_labels, './denoised_fastnlmeans/', './edges/', low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>actual_digit_count</th>\n",
       "      <th>./grayscaled/</th>\n",
       "      <th>./resized/</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1500</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00000.png</td>\n",
       "      <td>./resized/00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>5743</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00001.png</td>\n",
       "      <td>./resized/00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1056</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00002.png</td>\n",
       "      <td>./resized/00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00003.png</td>\n",
       "      <td>./resized/00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00004.png</td>\n",
       "      <td>./resized/00004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11714</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...</td>\n",
       "      <td>100000</td>\n",
       "      <td>6</td>\n",
       "      <td>./grayscaled/11714.png</td>\n",
       "      <td>./resized/11714.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11715</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...</td>\n",
       "      <td>85000</td>\n",
       "      <td>5</td>\n",
       "      <td>./grayscaled/11715.png</td>\n",
       "      <td>./resized/11715.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>./grayscaled/11716.png</td>\n",
       "      <td>./resized/11716.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11717</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...</td>\n",
       "      <td>13356</td>\n",
       "      <td>5</td>\n",
       "      <td>./grayscaled/11717.png</td>\n",
       "      <td>./resized/11717.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11718</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...</td>\n",
       "      <td>100000</td>\n",
       "      <td>6</td>\n",
       "      <td>./grayscaled/11718.png</td>\n",
       "      <td>./resized/11718.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11719 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_path   label  \\\n",
       "0      ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...    1500   \n",
       "1      ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...    5743   \n",
       "2      ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...    1056   \n",
       "3      ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...    1000   \n",
       "4      ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...    1000   \n",
       "...                                                  ...     ...   \n",
       "11714  ./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...  100000   \n",
       "11715  ./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...   85000   \n",
       "11716  ./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...  150000   \n",
       "11717  ./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...   13356   \n",
       "11718  ./ORAND-CAR-2014/CAR-B/b_train_images/b_car_00...  100000   \n",
       "\n",
       "       actual_digit_count           ./grayscaled/           ./resized/  \n",
       "0                       4  ./grayscaled/00000.png  ./resized/00000.png  \n",
       "1                       4  ./grayscaled/00001.png  ./resized/00001.png  \n",
       "2                       4  ./grayscaled/00002.png  ./resized/00002.png  \n",
       "3                       4  ./grayscaled/00003.png  ./resized/00003.png  \n",
       "4                       4  ./grayscaled/00004.png  ./resized/00004.png  \n",
       "...                   ...                     ...                  ...  \n",
       "11714                   6  ./grayscaled/11714.png  ./resized/11714.png  \n",
       "11715                   5  ./grayscaled/11715.png  ./resized/11715.png  \n",
       "11716                   6  ./grayscaled/11716.png  ./resized/11716.png  \n",
       "11717                   5  ./grayscaled/11717.png  ./resized/11717.png  \n",
       "11718                   6  ./grayscaled/11718.png  ./resized/11718.png  \n",
       "\n",
       "[11719 rows x 5 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_df = get_size_dataFrame(all_labels,'./grayscaled/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11719.000000</td>\n",
       "      <td>11719.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>172.944705</td>\n",
       "      <td>54.925250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>44.082070</td>\n",
       "      <td>9.066699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>144.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>199.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>363.000000</td>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              width        height\n",
       "count  11719.000000  11719.000000\n",
       "mean     172.944705     54.925250\n",
       "std       44.082070      9.066699\n",
       "min       40.000000     28.000000\n",
       "25%      144.000000     49.000000\n",
       "50%      171.000000     54.000000\n",
       "75%      199.000000     61.000000\n",
       "max      363.000000    108.000000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9mklEQVR4nO3dfXzO9f////uxc8ZOMDvRjM3p5CyKQ+mEZWkpUe/qu7JyUnlvhBLekbNq0olSQ97vUL2TUtE75+e8y4ihnERIeMu2ohnKxvb8/dHP8XHYhs04dry6XS+X43Lxej6fx+t4PHc03T1fZzZjjBEAAIBFebi6AAAAgCuJsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAPgqjhz5oyeffZZRUZGysPDQ127dnVJHTabTaNGjXLJZ7vKo48+qjp16ri6DMBlCDvAVbR371498cQTio6Olp+fnwICAnTjjTfqzTff1B9//OHq8iRJkyZN0owZM8p9v9OmTdMrr7yi++67T++9954GDhxY4tg6derorrvuKrZv48aNstlsV6RGV9u0aZNsNpuGDx9e4pjdu3fLZrNp0KBBV7EywL15uboA4K9i/vz5uv/+++Xr66sePXro2muvVX5+vr766isNHjxY27dv19SpU11dpiZNmqQaNWro0UcfLdf9rlixQrVq1dKECRPKdb+l9ccff8jLq2L+1XfdddepUaNG+uijj/TCCy8UO2bmzJmSpIcffvhqlga4tYr5Gw9YzL59+/Tggw8qKipKK1asUHh4uKMvOTlZe/bs0fz5811Y4ZWXnZ2toKAgV5chPz8/V5dwQYmJiRoxYoTWrVuntm3bFun/6KOP1KhRI1133XUuqA5wTxzGAq6C8ePH68SJE3r33Xedgs5Z9erV01NPPeXYPnPmjMaOHauYmBj5+vqqTp06+sc//qG8vDyn95V0/kmdOnWcVmZmzJghm82mr7/+WoMGDVJISIj8/f1177336pdffnF63/bt27V69WrZbDbZbDbdeuutF5zbyZMn9fTTTysyMlK+vr5q2LChXn31VRljJEk//fSTbDabVq5cqe3btzv2u2rVqov/4C7Ro48+qipVqujQoUPq2rWrqlSpopCQED3zzDMqKChwGlvcz+yrr77S9ddfLz8/P8XExOidd97RqFGjZLPZHGPOzqO4w2fF7fPQoUPq2bOnQkND5evrqyZNmmjatGkXnUtiYqKk/1vBOVdGRoZ27drlGPPFF18oISFBERER8vX1VUxMjMaOHVtkzudbtWpVsd9BSXPcuXOn7rvvPlWrVk1+fn5q3bq1/vOf/ziNOX36tEaPHq369evLz89P1atX10033aSlS5dedM7AlcbKDnAVfPnll4qOjla7du0uaXzv3r313nvv6b777tPTTz+t9evXKzU1Vd9//73mzJlT5jr69eun4OBgjRw5Uj/99JPeeOMNpaSk6OOPP5YkvfHGG+rXr5+qVKmi5557TpIUGhpa4v6MMbr77ru1cuVK9erVSy1atNDixYs1ePBgHTp0SBMmTFBISIg++OADvfjiizpx4oRSU1MlSY0bNy7zPIpTUFCg+Ph4tWnTRq+++qqWLVum1157TTExMerbt2+J79u6das6deqkkJAQjRo1SmfOnNHIkSMvOO+LycrKUtu2bWWz2ZSSkqKQkBAtXLhQvXr1Um5urgYMGFDie+vWrat27drpk08+0YQJE+Tp6enoOxuA/t//+3+S/gyxVapU0aBBg1SlShWtWLFCzz//vHJzc/XKK6+Uuf5zbd++XTfeeKNq1aqloUOHyt/fX5988om6du2qzz77TPfee68kadSoUUpNTVXv3r11ww03KDc3Vxs3btSmTZt0++23l0stQJkZAFfUsWPHjCRzzz33XNL4LVu2GEmmd+/eTu3PPPOMkWRWrFjhaJNkRo4cWWQfUVFRJikpybE9ffp0I8nExcWZwsJCR/vAgQONp6enycnJcbQ1adLE3HLLLZdU69y5c40k88ILLzi133fffcZms5k9e/Y42m655RbTpEmTS9pvVFSUSUhIKLZvw4YNRpKZPn26oy0pKclIMmPGjHEa27JlS9OqVSuntvN/Zl27djV+fn5m//79jrYdO3YYT09Pc+5fkfv27SvyuSXts1evXiY8PNz8+uuvTuMefPBBExgYaH7//feSpm6MMSYtLc1IMosXL3a0FRQUmFq1ahm73e5oK24/TzzxhKlcubI5deqUoy0pKclERUU5tleuXGkkmZUrVzq9t7g5duzY0TRt2tRpf4WFhaZdu3amfv36jrbmzZuX+J0BrsZhLOAKy83NlSRVrVr1ksYvWLBAkopcbfP0009L0mWd2/P44487HZpp3769CgoKtH///jLtb8GCBfL09FT//v2L1GqM0cKFC8tca1k8+eSTTtvt27fXjz/+WOL4goICLV68WF27dlXt2rUd7Y0bN1Z8fHyZajDG6LPPPlOXLl1kjNGvv/7qeMXHx+vYsWPatGnTBffxwAMPyNvb2+lQ1urVq3Xo0CHHISxJqlSpkuPPx48f16+//qr27dvr999/186dO8tU/7mOHj2qFStW6G9/+5tj/7/++quOHDmi+Ph47d69W4cOHZIkBQUFafv27dq9e/dlfy5Q3gg7wBUWEBAg6c//GV2K/fv3y8PDQ/Xq1XNqDwsLU1BQUJmDiSSn/6FLUnBwsCTpt99+K9P+9u/fr4iIiCJB7uwhqsup9WLODW3Snyceh4SEOLUFBwdfcG6//PKL/vjjD9WvX79IX8OGDctU1y+//KKcnBxNnTpVISEhTq/HHntM0p8na19I9erVFR8frzlz5ujUqVOS/jyE5eXlpb/97W+Ocdu3b9e9996rwMBABQQEKCQkxHGV1rFjx8pU/7n27NkjY4xGjBhRZC4jR450msuYMWOUk5OjBg0aqGnTpho8eLC+++67y64BKA+cswNcYQEBAYqIiNC2bdtK9b7z/2deGiWdoHru+R/nMv//ycQVhZ+fX4n3Hfr9998dY85V0tzKS0nfx/k/68LCQkl/XhqelJRU7HuaNWt20c97+OGHNW/ePM2bN0933323PvvsM8e5RZKUk5OjW265RQEBARozZoxiYmLk5+enTZs2aciQIY46ymMuzzzzTIkrXWdD+c0336y9e/fqiy++0JIlS/Svf/1LEyZM0JQpU9S7d++Lzhe4kgg7wFVw1113aerUqUpPT5fdbr/g2KioKBUWFmr37t1OJ/FmZWUpJydHUVFRjrbg4GDl5OQ4vT8/P1+HDx8uc62lCVlRUVFatmyZjh8/7rS6c/YQyrm1lkZUVJR27NhRbN+uXbsua9/nCgkJUaVKlYo99HL2c846uwp2/s/7/NWrkJAQVa1aVQUFBYqLiytzbXfffbeqVq2qmTNnytvbW7/99pvTIaxVq1bpyJEj+vzzz3XzzTc72vft23fRfV/qXKKjoyVJ3t7elzSXatWq6bHHHtNjjz2mEydO6Oabb9aoUaMIO3A5DmMBV8Gzzz4rf39/9e7dW1lZWUX69+7dqzfffFOSdOedd0r688qoc73++uuSpISEBEdbTEyM1qxZ4zRu6tSpF730+EL8/f2L/E+wJHfeeacKCgr09ttvO7VPmDBBNptNnTt3LlMNd955p/73v/9p7ty5Tu15eXn617/+pZo1a5bLfWY8PT0VHx+vuXPn6sCBA47277//XosXL3YaGxAQoBo1ahT5eU+aNKnIPrt3767PPvus2NW8cy/1v5BKlSrp3nvv1YIFCzR58mT5+/vrnnvucfocyXlVLj8/v0g9xYmKipKnp+dF51KzZk3deuuteuedd4oN0OfO5ciRI059VapUUb169YrcLgFwBVZ2gKsgJiZGM2fO1AMPPKDGjRs73UF57dq1mj17tuO+OM2bN1dSUpKmTp3qOFTxzTff6L333lPXrl112223Ofbbu3dvPfnkk+revbtuv/12ffvtt1q8eLFq1KhR5lpbtWqlyZMn64UXXlC9evVUs2ZNdejQodixXbp00W233abnnntOP/30k5o3b64lS5boiy++0IABAxQTE1OmGh5//HFNmzZN999/v3r27KmWLVvqyJEj+vjjj7Vt2za9//778vHxKfMczzV69GgtWrRI7du319///nedOXNGb731lpo0aVLknJPevXtr3Lhx6t27t1q3bq01a9bohx9+KLLPcePGaeXKlWrTpo369Omj2NhYHT16VJs2bdKyZct09OjRS6rt4Ycf1vvvv6/FixcrMTFR/v7+jr527dopODhYSUlJ6t+/v2w2mz744INLOiQZGBio+++/X2+99ZZsNptiYmI0b968Ys8lSktL00033aSmTZuqT58+io6OVlZWltLT0/W///1P3377rSQpNjZWt956q1q1aqVq1app48aN+vTTT5WSknJJcwWuKBdeCQb85fzwww+mT58+pk6dOsbHx8dUrVrV3Hjjjeatt95yurT39OnTZvTo0aZu3brG29vbREZGmmHDhjmNMebPy5GHDBliatSoYSpXrmzi4+PNnj17Srz0fMOGDU7vL+4S5MzMTJOQkGCqVq1qJF30MvTjx4+bgQMHmoiICOPt7W3q169vXnnlFadL3I0p3aXnxhjz22+/mYEDBzp+BgEBAea2224zCxcuLDI2KSnJ+Pv7F2kfOXKkOf+vORVzuf7q1atNq1atjI+Pj4mOjjZTpkwp9r2///676dWrlwkMDDRVq1Y1f/vb30x2dnax+8zKyjLJyckmMjLSeHt7m7CwMNOxY0czderUS/4ZnDlzxoSHhxtJZsGCBUX6v/76a9O2bVtTqVIlExERYZ599lmzePHiIt/p+ZeeG2PML7/8Yrp3724qV65sgoODzRNPPGG2bdtW7OX1e/fuNT169DBhYWHG29vb1KpVy9x1113m008/dYx54YUXzA033GCCgoJMpUqVTKNGjcyLL75o8vPzL3m+wJViM6aCnZkIABXAqFGjNHr06Ap38jaA0uOcHQAAYGmEHQAAYGmEHQAAYGmcswMAACyNlR0AAGBphB0AAGBp3FRQfz7/5eeff1bVqlUv63lEAADg6jHG6Pjx44qIiJCHR8nrN4QdST///LMiIyNdXQYAACiDgwcP6pprrimxn7AjOR5gePDgQQUEBLi4GgAAcClyc3MVGRnp9CDi4hB29H9PeQ4ICCDsAADgZi52CgonKAMAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEvzcnUBAP5PnaHznbZ/GpfgokoAwDpY2QEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJbGpefAFcbl5ADgWqzsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS/NydQEALqzO0PlF2n4al+CCSgDAPbGyAwAALI2wAwAALI2wAwAALI1zdgA3df65PJzHAwDFY2UHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYGmEHAABYmkvDzqhRo2Sz2ZxejRo1cvSfOnVKycnJql69uqpUqaLu3bsrKyvLaR8HDhxQQkKCKleurJo1a2rw4ME6c+bM1Z4KAACooFx+B+UmTZpo2bJljm0vr/8raeDAgZo/f75mz56twMBApaSkqFu3bvr6668lSQUFBUpISFBYWJjWrl2rw4cPq0ePHvL29tZLL7101ecCAAAqHpeHHS8vL4WFhRVpP3bsmN59913NnDlTHTp0kCRNnz5djRs31rp169S2bVstWbJEO3bs0LJlyxQaGqoWLVpo7NixGjJkiEaNGiUfH5+rPR3gkpz/qAeJxz0AwJXi8nN2du/erYiICEVHRysxMVEHDhyQJGVkZOj06dOKi4tzjG3UqJFq166t9PR0SVJ6erqaNm2q0NBQx5j4+Hjl5uZq+/btJX5mXl6ecnNznV4AAMCaXBp22rRpoxkzZmjRokWaPHmy9u3bp/bt2+v48ePKzMyUj4+PgoKCnN4TGhqqzMxMSVJmZqZT0Dnbf7avJKmpqQoMDHS8IiMjy3diAACgwnDpYazOnTs7/tysWTO1adNGUVFR+uSTT1SpUqUr9rnDhg3ToEGDHNu5ubkEHgAALMrlh7HOFRQUpAYNGmjPnj0KCwtTfn6+cnJynMZkZWU5zvEJCwsrcnXW2e3izgM6y9fXVwEBAU4vAABgTRUq7Jw4cUJ79+5VeHi4WrVqJW9vby1fvtzRv2vXLh04cEB2u12SZLfbtXXrVmVnZzvGLF26VAEBAYqNjb3q9QMAgIrHpYexnnnmGXXp0kVRUVH6+eefNXLkSHl6euqhhx5SYGCgevXqpUGDBqlatWoKCAhQv379ZLfb1bZtW0lSp06dFBsbq0ceeUTjx49XZmamhg8fruTkZPn6+rpyagAAoIJwadj53//+p4ceekhHjhxRSEiIbrrpJq1bt04hISGSpAkTJsjDw0Pdu3dXXl6e4uPjNWnSJMf7PT09NW/ePPXt21d2u13+/v5KSkrSmDFjXDUlAABQwbg07MyaNeuC/X5+fkpLS1NaWlqJY6KiorRgwYLyLg1wS9y/BwCKqlDn7AAAAJQ3l99BGbCK81dVWFEBgIqBlR0AAGBprOwAfwGsOgH4K2NlBwAAWBphBwAAWBphBwAAWBrn7AClxL1sAMC9sLIDAAAsjbADAAAsjbADAAAsjbADAAAsjROUgQvgZnwA4P5Y2QEAAJZG2AEAAJbGYSzgL4zDdAD+CljZAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlubl6gKAiqDO0PlF2n4al+CCSgAA5Y2VHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGkVJuyMGzdONptNAwYMcLSdOnVKycnJql69uqpUqaLu3bsrKyvL6X0HDhxQQkKCKleurJo1a2rw4ME6c+bMVa4eAABUVF6uLkCSNmzYoHfeeUfNmjVzah84cKDmz5+v2bNnKzAwUCkpKerWrZu+/vprSVJBQYESEhIUFhamtWvX6vDhw+rRo4e8vb310ksvuWIqgNurM3R+kbafxiW4oBIAKB8uX9k5ceKEEhMT9c9//lPBwcGO9mPHjundd9/V66+/rg4dOqhVq1aaPn261q5dq3Xr1kmSlixZoh07dujf//63WrRooc6dO2vs2LFKS0tTfn6+q6YEAAAqEJeHneTkZCUkJCguLs6pPSMjQ6dPn3Zqb9SokWrXrq309HRJUnp6upo2barQ0FDHmPj4eOXm5mr79u0lfmZeXp5yc3OdXgAAwJpcehhr1qxZ2rRpkzZs2FCkLzMzUz4+PgoKCnJqDw0NVWZmpmPMuUHnbP/ZvpKkpqZq9OjRl1k9AABwBy5b2Tl48KCeeuopffjhh/Lz87uqnz1s2DAdO3bM8Tp48OBV/XwAAHD1uCzsZGRkKDs7W9ddd528vLzk5eWl1atXa+LEifLy8lJoaKjy8/OVk5Pj9L6srCyFhYVJksLCwopcnXV2++yY4vj6+iogIMDpBQAArMllYadjx47aunWrtmzZ4ni1bt1aiYmJjj97e3tr+fLljvfs2rVLBw4ckN1ulyTZ7XZt3bpV2dnZjjFLly5VQECAYmNjr/qcAABAxeOyc3aqVq2qa6+91qnN399f1atXd7T36tVLgwYNUrVq1RQQEKB+/frJbrerbdu2kqROnTopNjZWjzzyiMaPH6/MzEwNHz5cycnJ8vX1vepzAgAAFU+FuM9OSSZMmCAPDw91795deXl5io+P16RJkxz9np6emjdvnvr27Su73S5/f38lJSVpzJgxLqwaAABUJBUq7Kxatcpp28/PT2lpaUpLSyvxPVFRUVqwYMEVrgwAALgrl99nBwAA4Eoi7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEurUE89B1Bx1Rk632n7p3EJLqoEAEqHlR0AAGBphB0AAGBpHMbCXw6HYwDgr4WVHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGllCjvR0dE6cuRIkfacnBxFR0dfdlEAAADlxassb/rpp59UUFBQpD0vL0+HDh267KIAuIc6Q+cXaftpXIILKgGAkpUq7PznP/9x/Hnx4sUKDAx0bBcUFGj58uWqU6dOuRUHAABwuUoVdrp27SpJstlsSkpKcurz9vZWnTp19Nprr5VbcQAAAJerVGGnsLBQklS3bl1t2LBBNWrUuCJFAQAAlJcynbOzb9++8q4DgIWcfy4P5/EAcKUyhR1JWr58uZYvX67s7GzHis9Z06ZNu+zCAAAAykOZws7o0aM1ZswYtW7dWuHh4bLZbOVdFwAAQLkoU9iZMmWKZsyYoUceeaS86wEAAChXZbqpYH5+vtq1a1fetQAAAJS7Mq3s9O7dWzNnztSIESPKux6g3HDDOwCAVMawc+rUKU2dOlXLli1Ts2bN5O3t7dT/+uuvl0txAAAAl6tMh7G+++47tWjRQh4eHtq2bZs2b97seG3ZsuWS9zN58mQ1a9ZMAQEBCggIkN1u18KFCx39p06dUnJysqpXr64qVaqoe/fuysrKctrHgQMHlJCQoMqVK6tmzZoaPHiwzpw5U5ZpAQAACyrTys7KlSvL5cOvueYajRs3TvXr15cxRu+9957uuecebd68WU2aNNHAgQM1f/58zZ49W4GBgUpJSVG3bt309ddfS/rzERUJCQkKCwvT2rVrdfjwYfXo0UPe3t566aWXyqVGAADg3sp8n53y0KVLF6ftF198UZMnT9a6det0zTXX6N1339XMmTPVoUMHSdL06dPVuHFjrVu3Tm3bttWSJUu0Y8cOLVu2TKGhoWrRooXGjh2rIUOGaNSoUfLx8XHFtAAAQAVSprBz2223XfDeOitWrCj1PgsKCjR79mydPHlSdrtdGRkZOn36tOLi4hxjGjVqpNq1ays9PV1t27ZVenq6mjZtqtDQUMeY+Ph49e3bV9u3b1fLli2L/ay8vDzl5eU5tnNzc0tdLwAAcA9lCjstWrRw2j59+rS2bNmibdu2FXlA6MVs3bpVdrtdp06dUpUqVTRnzhzFxsZqy5Yt8vHxUVBQkNP40NBQZWZmSpIyMzOdgs7Z/rN9JUlNTdXo0aNLVScAAHBPZQo7EyZMKLZ91KhROnHiRKn21bBhQ23ZskXHjh3Tp59+qqSkJK1evbosZV2yYcOGadCgQY7t3NxcRUZGXtHPBAAArlGmq7FK8vDDD5f6uVg+Pj6qV6+eWrVqpdTUVDVv3lxvvvmmwsLClJ+fr5ycHKfxWVlZCgsLkySFhYUVuTrr7PbZMcXx9fV1XAF29gUAAKypXMNOenq6/Pz8LmsfhYWFysvLU6tWreTt7a3ly5c7+nbt2qUDBw7IbrdLkux2u7Zu3ars7GzHmKVLlyogIECxsbGXVQcAALCGMh3G6tatm9O2MUaHDx/Wxo0bS3VX5WHDhqlz586qXbu2jh8/rpkzZ2rVqlVavHixAgMD1atXLw0aNEjVqlVTQECA+vXrJ7vdrrZt20qSOnXqpNjYWD3yyCMaP368MjMzNXz4cCUnJ8vX17csUwMAABZTprATGBjotO3h4aGGDRtqzJgx6tSp0yXvJzs7Wz169NDhw4cVGBioZs2aafHixbr99tsl/XlukIeHh7p37668vDzFx8dr0qRJjvd7enpq3rx56tu3r+x2u/z9/ZWUlKQxY8aUZVoArrDzH+HB4zsAXA1lCjvTp08vlw9/9913L9jv5+entLQ0paWllTgmKipKCxYsKJd6AACA9VzWTQUzMjL0/fffS5KaNGlS4n1tAAAAXKVMYSc7O1sPPvigVq1a5bgPTk5Ojm677TbNmjVLISEh5VkjAABAmZXpaqx+/frp+PHj2r59u44ePaqjR49q27Ztys3NVf/+/cu7RgAAgDIr08rOokWLtGzZMjVu3NjRFhsbq7S0tFKdoAwAAHCllWllp7CwUN7e3kXavb29VVhYeNlFAQAAlJcyhZ0OHTroqaee0s8//+xoO3TokAYOHKiOHTuWW3EAAACXq0xh5+2331Zubq7q1KmjmJgYxcTEqG7dusrNzdVbb71V3jUCAACUWZnO2YmMjNSmTZu0bNky7dy5U5LUuHFjxcXFlWtxAKzv/BsNStxsEED5KtXKzooVKxQbG6vc3FzZbDbdfvvt6tevn/r166frr79eTZo00X//+98rVSsAAECplSrsvPHGG+rTp0+xTwkPDAzUE088oddff73cigMAALhcpQo73377re64444S+zt16qSMjIzLLgoAAKC8lOqcnaysrGIvOXfszMtLv/zyy2UXBZQG53wAAC6kVCs7tWrV0rZt20rs/+677xQeHn7ZRQEAAJSXUoWdO++8UyNGjNCpU6eK9P3xxx8aOXKk7rrrrnIrDgAA4HKV6jDW8OHD9fnnn6tBgwZKSUlRw4YNJUk7d+5UWlqaCgoK9Nxzz12RQgEAAMqiVGEnNDRUa9euVd++fTVs2DAZYyRJNptN8fHxSktLU2ho6BUpFAAAoCxKfVPBqKgoLViwQL/99pv27NkjY4zq16+v4ODgK1EfAADAZSnTHZQlKTg4WNdff3151gIAAFDuyvRsLAAAAHdB2AEAAJZG2AEAAJZG2AEAAJZG2AEAAJZW5quxAOBKOv+ZZzzvDEBZsbIDAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjaeew63wJGwAQGmxsgMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyN++wAcBvn32dJ4l5LAC6OlR0AAGBphB0AAGBphB0AAGBphB0AAGBphB0AAGBpLg07qampuv7661W1alXVrFlTXbt21a5du5zGnDp1SsnJyapevbqqVKmi7t27Kysry2nMgQMHlJCQoMqVK6tmzZoaPHiwzpw5czWnAgAAKiiXhp3Vq1crOTlZ69at09KlS3X69Gl16tRJJ0+edIwZOHCgvvzyS82ePVurV6/Wzz//rG7dujn6CwoKlJCQoPz8fK1du1bvvfeeZsyYoeeff94VUwIAABWMS++zs2jRIqftGTNmqGbNmsrIyNDNN9+sY8eO6d1339XMmTPVoUMHSdL06dPVuHFjrVu3Tm3bttWSJUu0Y8cOLVu2TKGhoWrRooXGjh2rIUOGaNSoUfLx8XHF1AAAQAVRoc7ZOXbsmCSpWrVqkqSMjAydPn1acXFxjjGNGjVS7dq1lZ6eLklKT09X06ZNFRoa6hgTHx+v3Nxcbd++/SpWDwAAKqIKcwflwsJCDRgwQDfeeKOuvfZaSVJmZqZ8fHwUFBTkNDY0NFSZmZmOMecGnbP9Z/uKk5eXp7y8PMd2bm5ueU0DAABUMBVmZSc5OVnbtm3TrFmzrvhnpaamKjAw0PGKjIy84p8JAABco0KEnZSUFM2bN08rV67UNddc42gPCwtTfn6+cnJynMZnZWUpLCzMMeb8q7PObp8dc75hw4bp2LFjjtfBgwfLcTYAAKAicWnYMcYoJSVFc+bM0YoVK1S3bl2n/latWsnb21vLly93tO3atUsHDhyQ3W6XJNntdm3dulXZ2dmOMUuXLlVAQIBiY2OL/VxfX18FBAQ4vQAAgDW59Jyd5ORkzZw5U1988YWqVq3qOMcmMDBQlSpVUmBgoHr16qVBgwapWrVqCggIUL9+/WS329W2bVtJUqdOnRQbG6tHHnlE48ePV2ZmpoYPH67k5GT5+vq6cnq4DDzdGgBQXlwadiZPnixJuvXWW53ap0+frkcffVSSNGHCBHl4eKh79+7Ky8tTfHy8Jk2a5Bjr6empefPmqW/fvrLb7fL391dSUpLGjBlztaYBAAAqMJeGHWPMRcf4+fkpLS1NaWlpJY6JiorSggULyrM0AG7k/JXAn8YlsDoIwKFCnKAMAABwpRB2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApRF2AACApXm5ugD8ddQZOr9I20/jElxQCQDgr4SVHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGmEHQAAYGncVBAud/7NBrnRIACgPLGyAwAALI2wAwAALI2wAwAALI2wAwAALI0TlAH8pXBCPPDXw8oOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNJ56DgDiaeiAlbGyAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALM2lYWfNmjXq0qWLIiIiZLPZNHfuXKd+Y4yef/55hYeHq1KlSoqLi9Pu3budxhw9elSJiYkKCAhQUFCQevXqpRMnTlzFWQAAgIrMpWHn5MmTat68udLS0ortHz9+vCZOnKgpU6Zo/fr18vf3V3x8vE6dOuUYk5iYqO3bt2vp0qWaN2+e1qxZo8cff/xqTQEAAFRwLr2pYOfOndW5c+di+4wxeuONNzR8+HDdc889kqT3339foaGhmjt3rh588EF9//33WrRokTZs2KDWrVtLkt566y3deeedevXVVxUREXHV5gIAACqmCnvOzr59+5SZmam4uDhHW2BgoNq0aaP09HRJUnp6uoKCghxBR5Li4uLk4eGh9evXX/WaAQBAxVNhHxeRmZkpSQoNDXVqDw0NdfRlZmaqZs2aTv1eXl6qVq2aY0xx8vLylJeX59jOzc0tr7IBAEAFU2HDzpWUmpqq0aNHu7oMAG6K52gB7qXCHsYKCwuTJGVlZTm1Z2VlOfrCwsKUnZ3t1H/mzBkdPXrUMaY4w4YN07FjxxyvgwcPlnP1AACgoqiwKzt169ZVWFiYli9frhYtWkj683DT+vXr1bdvX0mS3W5XTk6OMjIy1KpVK0nSihUrVFhYqDZt2pS4b19fX/n6+l7xOQBwb+ev4Eis4gDuyKVh58SJE9qzZ49je9++fdqyZYuqVaum2rVra8CAAXrhhRdUv3591a1bVyNGjFBERIS6du0qSWrcuLHuuOMO9enTR1OmTNHp06eVkpKiBx98kCuxAACAJBeHnY0bN+q2225zbA8aNEiSlJSUpBkzZujZZ5/VyZMn9fjjjysnJ0c33XSTFi1aJD8/P8d7PvzwQ6WkpKhjx47y8PBQ9+7dNXHixKs+FwAAUDG5NOzceuutMsaU2G+z2TRmzBiNGTOmxDHVqlXTzJkzr0R5AADAAirsCcoAAADlgbADAAAsrcJejQX3xn1IAAAVBSs7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0rgaCwDKAc/RAiouVnYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClcVNBXJbzb6TGTdQAABUNKzsAAMDSWNnBJeFW+AAAd8XKDgAAsDTCDgAAsDTCDgAAsDTO2QGAK4grFgHXY2UHAABYGis7AHCVcXUjcHURdlAEy+4AACsh7ABABcGKD3BlEHYAoIJjtRW4PJygDAAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI1nYwGAG+KhocClI+z8RRT3IEH+sgSsh4eGAkVxGAsAAFgaKzsAYHGs4uKvjpUdAABgaYQdAABgaRzGAoC/KE5mxl8FYQcA4IQQBKvhMBYAALA0VnYAABdV0hVdrALBHRB2AADlikvdUdFYJuykpaXplVdeUWZmppo3b6633npLN9xwg6vLuur4SwYAAGeWOGfn448/1qBBgzRy5Eht2rRJzZs3V3x8vLKzs11dGgAAcDFLrOy8/vrr6tOnjx577DFJ0pQpUzR//nxNmzZNQ4cOdXF1Vw7HygG4E57RB1dx+7CTn5+vjIwMDRs2zNHm4eGhuLg4paenu7AyAEBZlfSPudIEptL8g5B/PFqb24edX3/9VQUFBQoNDXVqDw0N1c6dO4t9T15envLy8hzbx44dkyTl5uaWe33XjlxcpG3b6PhSjT2//ez7C/N+d2rPzc0t0lZS+5UcW1Fru9qfV5F/Fq6o7Wp/nhVqu9qfV5F/FuVVW0l/z5b0ecX9/Vvav6v/iq7Wz+Ls92SMufBA4+YOHTpkJJm1a9c6tQ8ePNjccMMNxb5n5MiRRhIvXrx48eLFywKvgwcPXjAruP3KTo0aNeTp6amsrCyn9qysLIWFhRX7nmHDhmnQoEGO7cLCQh09elTVq1eXzWa7ovVeitzcXEVGRurgwYMKCAhwdTnlyspzk5ifu2N+7svKc5OYX0mMMTp+/LgiIiIuOM7tw46Pj49atWql5cuXq2vXrpL+DC/Lly9XSkpKse/x9fWVr6+vU1tQUNAVrrT0AgICLPkftWTtuUnMz90xP/dl5blJzK84gYGBFx3j9mFHkgYNGqSkpCS1bt1aN9xwg9544w2dPHnScXUWAAD467JE2HnggQf0yy+/6Pnnn1dmZqZatGihRYsWFTlpGQAA/PVYIuxIUkpKSomHrdyNr6+vRo4cWeRQmxVYeW4S83N3zM99WXluEvO7XDZjLna9FgAAgPuyxOMiAAAASkLYAQAAlkbYAQAAlkbYAQAAlkbYcZHJkyerWbNmjhso2e12LVy40NF/6tQpJScnq3r16qpSpYq6d+9e5C7R7mLcuHGy2WwaMGCAo83d5zdq1CjZbDanV6NGjRz97j6/Q4cO6eGHH1b16tVVqVIlNW3aVBs3bnT0G2P0/PPPKzw8XJUqVVJcXJx2797twoovXZ06dYp8dzabTcnJyZLc/7srKCjQiBEjVLduXVWqVEkxMTEaO3as07OD3Pn7O378uAYMGKCoqChVqlRJ7dq104YNGxz97ja3NWvWqEuXLoqIiJDNZtPcuXOd+i9lPkePHlViYqICAgIUFBSkXr166cSJE1dxFsW72Nw+//xzderUyfH0gi1bthTZR3n9PhJ2XOSaa67RuHHjlJGRoY0bN6pDhw665557tH37dknSwIED9eWXX2r27NlavXq1fv75Z3Xr1s3FVZfehg0b9M4776hZs2ZO7VaYX5MmTXT48GHH66uvvnL0ufP8fvvtN914443y9vbWwoULtWPHDr322msKDg52jBk/frwmTpyoKVOmaP369fL391d8fLxOnTrlwsovzYYNG5y+t6VLl0qS7r//fknu/d1J0ssvv6zJkyfr7bff1vfff6+XX35Z48eP11tvveUY487fX+/evbV06VJ98MEH2rp1qzp16qS4uDgdOnRIkvvN7eTJk2revLnS0tKK7b+U+SQmJmr79u1aunSp5s2bpzVr1ujxxx+/WlMo0cXmdvLkSd100016+eWXS9xHuf0+XvaTOFFugoODzb/+9S+Tk5NjvL29zezZsx1933//vZFk0tPTXVhh6Rw/ftzUr1/fLF261Nxyyy3mqaeeMsYYS8xv5MiRpnnz5sX2ufv8hgwZYm666aYS+wsLC01YWJh55ZVXHG05OTnG19fXfPTRR1ejxHL11FNPmZiYGFNYWOj2350xxiQkJJiePXs6tXXr1s0kJiYaY9z7+/v999+Np6enmTdvnlP7ddddZ5577jm3npsxxkgyc+bMcWxfynx27NhhJJkNGzY4xixcuNDYbDZz6NChq1b7xZw/t3Pt27fPSDKbN292ai/P30dWdiqAgoICzZo1SydPnpTdbldGRoZOnz6tuLg4x5hGjRqpdu3aSk9Pd2GlpZOcnKyEhASneUiyzPx2796tiIgIRUdHKzExUQcOHJDk/vP7z3/+o9atW+v+++9XzZo11bJlS/3zn/909O/bt0+ZmZlO8wsMDFSbNm3cYn7nys/P17///W/17NlTNpvN7b87SWrXrp2WL1+uH374QZL07bff6quvvlLnzp0luff3d+bMGRUUFMjPz8+pvVKlSvrqq6/cem7FuZT5pKenKygoSK1bt3aMiYuLk4eHh9avX3/Vay5P5fn7aJk7KLujrVu3ym6369SpU6pSpYrmzJmj2NhYbdmyRT4+PkUeThoaGqrMzEzXFFtKs2bN0qZNm5yOpZ+VmZnp9vNr06aNZsyYoYYNG+rw4cMaPXq02rdvr23btrn9/H788UdNnjxZgwYN0j/+8Q9t2LBB/fv3l4+Pj5KSkhxzOP9xLO4yv3PNnTtXOTk5evTRRyVZ47/NoUOHKjc3V40aNZKnp6cKCgr04osvKjExUZLc+vurWrWq7Ha7xo4dq8aNGys0NFQfffSR0tPTVa9ePbeeW3EuZT6ZmZmqWbOmU7+Xl5eqVavmlnM+V3n+PhJ2XKhhw4basmWLjh07pk8//VRJSUlavXq1q8u6bAcPHtRTTz2lpUuXFvkXmFWc/VeyJDVr1kxt2rRRVFSUPvnkE1WqVMmFlV2+wsJCtW7dWi+99JIkqWXLltq2bZumTJmipKQkF1dXvt5991117txZERERri6l3HzyySf68MMPNXPmTDVp0kRbtmzRgAEDFBERYYnv74MPPlDPnj1Vq1YteXp66rrrrtNDDz2kjIwMV5eGCozDWC7k4+OjevXqqVWrVkpNTVXz5s315ptvKiwsTPn5+crJyXEan5WVpbCwMNcUWwoZGRnKzs7WddddJy8vL3l5eWn16tWaOHGivLy8FBoa6tbzK05QUJAaNGigPXv2uP33Fx4ertjYWKe2xo0bOw7TnZ3D+VdEuMv8ztq/f7+WLVum3r17O9rc/buTpMGDB2vo0KF68MEH1bRpUz3yyCMaOHCgUlNTJbn/9xcTE6PVq1frxIkTOnjwoL755hudPn1a0dHRbj+3813KfMLCwpSdne3Uf+bMGR09etQt53yu8vx9JOxUIIWFhcrLy1OrVq3k7e2t5cuXO/p27dqlAwcOyG63u7DCS9OxY0dt3bpVW7Zscbxat26txMREx5/deX7FOXHihPbu3avw8HC3//5uvPFG7dq1y6nthx9+UFRUlCSpbt26CgsLc5pfbm6u1q9f7xbzO2v69OmqWbOmEhISHG3u/t1J0u+//y4PD+e/2j09PVVYWCjJOt+fv7+/wsPD9dtvv2nx4sW65557LDO3sy5lPna7XTk5OU4rWytWrFBhYaHatGlz1WsuT+X6+1jaM6pRPoYOHWpWr15t9u3bZ7777jszdOhQY7PZzJIlS4wxxjz55JOmdu3aZsWKFWbjxo3Gbrcbu93u4qrL7tyrsYxx//k9/fTTZtWqVWbfvn3m66+/NnFxcaZGjRomOzvbGOPe8/vmm2+Ml5eXefHFF83u3bvNhx9+aCpXrmz+/e9/O8aMGzfOBAUFmS+++MJ899135p577jF169Y1f/zxhwsrv3QFBQWmdu3aZsiQIUX63Pm7M8aYpKQkU6tWLTNv3jyzb98+8/nnn5saNWqYZ5991jHGnb+/RYsWmYULF5off/zRLFmyxDRv3ty0adPG5OfnG2Pcb27Hjx83mzdvNps3bzaSzOuvv242b95s9u/fb4y5tPnccccdpmXLlmb9+vXmq6++MvXr1zcPPfSQq6bkcLG5HTlyxGzevNnMnz/fSDKzZs0ymzdvNocPH3bso7x+Hwk7LtKzZ08TFRVlfHx8TEhIiOnYsaMj6BhjzB9//GH+/ve/m+DgYFO5cmVz7733Ov0H4G7ODzvuPr8HHnjAhIeHGx8fH1OrVi3zwAMPmD179jj63X1+X375pbn22muNr6+vadSokZk6dapTf2FhoRkxYoQJDQ01vr6+pmPHjmbXrl0uqrb0Fi9ebCQVW7O7f3e5ubnmqaeeMrVr1zZ+fn4mOjraPPfccyYvL88xxp2/v48//thER0cbHx8fExYWZpKTk01OTo6j393mtnLlSiOpyCspKckYc2nzOXLkiHnooYdMlSpVTEBAgHnsscfM8ePHXTAbZxeb2/Tp04vtHzlypGMf5fX7aDPmnNtqAgAAWAzn7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AAAAEsj7AC4aurUqaM33njD1WWUi1GjRqlFixauLgPAJSDsALigW2+9VQMGDCjSPmPGDAUFBZVqXxs2bNDjjz9ePoWV0Wuvvabg4GCdOnWqSN/vv/+ugIAATZw40QWVAbhSCDsArpqQkBBVrlzZpTU88sgjOnnypD7//PMifZ9++qny8/P18MMPu6AyAFcKYQdAuXj00UfVtWtXvfrqqwoPD1f16tWVnJys06dPO8acfxhr9+7duvnmm+Xn56fY2FgtXbpUNptNc+fOlSStWrVKNptNOTk5jvds2bJFNptNP/30k6Ptq6++Uvv27VWpUiVFRkaqf//+OnnyZLF11qxZU126dNG0adOK9E2bNk1du3ZVtWrVNGTIEDVo0ECVK1dWdHS0RowY4TSX8xW3Ata1a1c9+uijju28vDw988wzqlWrlvz9/dWmTRutWrXK0b9//3516dJFwcHB8vf3V5MmTbRgwYISPxPApfFydQEArGPlypUKDw/XypUrtWfPHj3wwANq0aKF+vTpU2RsYWGhunXrptDQUK1fv17Hjh0r9nDZxezdu1d33HGHXnjhBU2bNk2//PKLUlJSlJKSounTpxf7nl69eumuu+7S/v37FRUVJUn68ccftWbNGi1evFiSVLVqVc2YMUMRERHaunWr+vTpo6pVq+rZZ58tdY1npaSkaMeOHZo1a5YiIiI0Z84c3XHHHdq6davq16+v5ORk5efna82aNfL399eOHTtUpUqVMn8egD+xsgOg3AQHB+vtt99Wo0aNdNdddykhIUHLly8vduyyZcu0c+dOvf/++2revLluvvlmvfTSS6X+zNTUVCUmJmrAgAGqX7++2rVrp4kTJ+r9998v9rwcSYqPj1dERIRTGJoxY4YiIyPVsWNHSdLw4cPVrl071alTR126dNEzzzyjTz75pNT1nXXgwAFNnz5ds2fPVvv27RUTE6NnnnlGN910k6OOAwcO6MYbb1TTpk0VHR2tu+66SzfffHOZPxPAn1jZAVBumjRpIk9PT8d2eHi4tm7dWuzY77//XpGRkYqIiHC02e32Un/mt99+q++++04ffviho80Yo8LCQu3bt0+NGzcu8h5PT08lJSVpxowZGjlypIwxeu+99/TYY4/Jw+PPfwN+/PHHmjhxovbu3asTJ07ozJkzCggIKHV9Z23dulUFBQVq0KCBU3teXp6qV68uSerfv7/69u2rJUuWKC4uTt27d1ezZs3K/JkA/kTYAXBBAQEBOnbsWJH2nJwcBQYGOrV5e3s7bdtsNhUWFpb5s88GD2OMo+3882ZOnDihJ554Qv379y/y/tq1a5e47549eyo1NVUrVqxQYWGhDh48qMcee0ySlJ6ersTERI0ePVrx8fEKDAzUrFmz9Nprr12w1nPrPL/WEydOyNPTUxkZGU6BUJLjUFXv3r0VHx+v+fPna8mSJUpNTdVrr72mfv36lfi5AC6OsAPggho2bKglS5YUad+0aVORVYrSaNy4sQ4ePKjDhw8rPDxckrRu3TqnMSEhIZKkw4cPKzg4WNKfJyif67rrrtOOHTtUr169Un1+TEyMbrnlFk2bNk3GGMXFxTnO31m7dq2ioqL03HPPOcbv37//gvsLCQnR4cOHHdsFBQXatm2bbrvtNklSy5YtVVBQoOzsbLVv377E/URGRurJJ5/Uk08+qWHDhumf//wnYQe4TJyzA+CC+vbtqx9++EH9+/fXd999p127dun111/XRx99pKeffrrM+42Li1ODBg2UlJSkb7/9Vv/973+dwoUk1atXT5GRkRo1apR2796t+fPnF1ldGTJkiNauXauUlBRt2bJFu3fv1hdffKGUlJSL1tCrVy99/vnnmjNnjnr16uVor1+/vg4cOKBZs2Zp7969mjhxoubMmXPBfXXo0EHz58/X/PnztXPnTvXt29fpKrIGDRooMTFRPXr00Oeff659+/bpm2++UWpqqubPny9JGjBggBYvXqx9+/Zp06ZNWrlyZbGH4QCUDmEHwAVFR0drzZo12rlzp+Li4tSmTRt98sknmj17tu64444y79fDw0Nz5szRH3/8oRtuuEG9e/fWiy++6DTG29tbH330kXbu3KlmzZrp5Zdf1gsvvOA0plmzZlq9erV++OEHtW/fXi1bttTzzz/vdC5QSbp37y5fX19VrlxZXbt2dbTffffdGjhwoFJSUtSiRQutXbtWI0aMuOC+evbsqaSkJPXo0UO33HKLoqOjHas6Z02fPl09evTQ008/rYYNG6pr167asGGD43BbQUGBkpOT1bhxY91xxx1q0KCBJk2adNF5ALgwmzn/IDMAuJDNZtOcOXOcwgcAXA5WdgAAgKURdgAAgKVxNRaACoUj6wDKGys7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0v4/Kl/hetOGcW4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heights = sizes_df['height'].value_counts()\n",
    "value_counts_df = heights.reset_index()\n",
    "value_counts_df.columns = ['Value', 'Count']\n",
    "plt.bar(heights.index, heights.values)\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Unique Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widths = sizes_df['width'].value_counts()\n",
    "value_counts_df = widths.reset_index()\n",
    "value_counts_df.columns = ['Value', 'Count']\n",
    "plt.bar(widths.index, widths.values)\n",
    "plt.xlabel('Unique Values')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Unique Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on these two plots its better to resize images to 53*173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits= '0123456789'\n",
    "img_h=54\n",
    "img_w=174\n",
    "#image Channels\n",
    "img_c=1\n",
    "\n",
    "# classes for softmax with number of letters +1 for blank space in ctc\n",
    "num_classes=len(digits) +1\n",
    "batch_size=64\n",
    "max_length=8 # considering max length of ground truths labels to be 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_numbers_labels(number):\n",
    "    \"\"\"\n",
    "    Encodes the Ground Truth Labels to a list of Values like eg.HAT returns [17,10,29]\n",
    "    \"\"\"\n",
    "    label_lst=[]\n",
    "    for char in str(number):\n",
    "        label_lst.append(digits.find(char)) # keeping 0 for blank and for padding labels\n",
    "    return label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_from_labels(labels):\n",
    "    txt=[]\n",
    "    for ele in labels:\n",
    "        if ele == len(digits): # CTC blank space\n",
    "            txt.append(\"\")\n",
    "        else:\n",
    "            #print(letters[ele])\n",
    "            txt.append(digits[ele])\n",
    "    return \"\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_function(args):\n",
    "    y_pred, y_true, input_length, label_length = args \n",
    "    y_pred = y_pred[:, 2:, :] # maybe need change based on my project and model architecture\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dense,MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, Flatten, Activation, Bidirectional\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Concatenate, Add, Multiply, Lambda\n",
    "from keras.layers import UpSampling2D, Reshape\n",
    "from keras.layers import add, concatenate\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,GRU\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_digit_recogniser_model_1(stage,drop_out_rate=0.35):\n",
    "    img_h=54\n",
    "    img_w=174\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_w, img_h)\n",
    "    else:\n",
    "        input_shape = (img_w, img_h, 1)\n",
    "        #input shape is (174, 54, 1)  \n",
    "        #input shape is (174, 54, 64)  \n",
    "        #input shape is (87, 27, 64)  \n",
    "        #input shape is (87, 27, 128)\n",
    "        #input shape is (43, 13, 128)\n",
    "        #input shape is (43, 13, 256)\n",
    "        #input shape is (43, 6, 512)\n",
    "        #input shape is (43, 3, 512)\n",
    "        #input shape is (43, 1536)\n",
    "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
    "\n",
    "    # Convolution layer \n",
    "    model = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(model_input) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max1')(model) \n",
    "\n",
    "    model = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max2')(model) \n",
    "\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 3), name='max3')(model)  \n",
    "\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv6')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max4')(model) \n",
    "\n",
    "    model = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(model)\n",
    "    model=Dropout(0.25)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)    \n",
    "\n",
    "    # CNN to RNN\n",
    "    model = Reshape(target_shape=(43, 1024), name='reshape')(model)\n",
    "\n",
    "    model = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(model)  \n",
    "\n",
    "    # RNN layer\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(model)\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model) \n",
    "    y_pred = Activation('softmax', name='softmax')(model)\n",
    "\n",
    "    \n",
    "    labels = Input(name='ground_truth_labels', shape=[max_length], dtype='float32') \n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
    "\n",
    "    #CTC loss function\n",
    "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "\n",
    "    if stage=='train':\n",
    "        return model_input,y_pred,Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[model_input], outputs=y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input,y_pred,img_digit_recog=Image_digit_recogniser_model_1('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func = K.function([model_input], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " img_input (InputLayer)      [(None, 174, 54, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)              (None, 174, 54, 64)          640       ['img_input[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 174, 54, 64)          256       ['conv1[0][0]']               \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 174, 54, 64)          0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max1 (MaxPooling2D)         (None, 87, 27, 64)           0         ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)              (None, 87, 27, 128)          73856     ['max1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 87, 27, 128)          512       ['conv2[0][0]']               \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 87, 27, 128)          0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max2 (MaxPooling2D)         (None, 43, 13, 128)          0         ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)              (None, 43, 13, 256)          295168    ['max2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 43, 13, 256)          1024      ['conv3[0][0]']               \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 43, 13, 256)          0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv4 (Conv2D)              (None, 43, 13, 256)          590080    ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 43, 13, 256)          0         ['conv4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 43, 13, 256)          1024      ['dropout_6[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 43, 13, 256)          0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max3 (MaxPooling2D)         (None, 43, 4, 256)           0         ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " conv5 (Conv2D)              (None, 43, 4, 512)           1180160   ['max3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 43, 4, 512)           2048      ['conv5[0][0]']               \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 43, 4, 512)           0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv6 (Conv2D)              (None, 43, 4, 512)           2359808   ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 43, 4, 512)           0         ['conv6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 43, 4, 512)           2048      ['dropout_7[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 43, 4, 512)           0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max4 (MaxPooling2D)         (None, 43, 2, 512)           0         ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " con7 (Conv2D)               (None, 43, 2, 512)           1049088   ['max4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 43, 2, 512)           0         ['con7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 43, 2, 512)           2048      ['dropout_8[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 43, 2, 512)           0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 43, 1024)             0         ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " dense1 (Dense)              (None, 43, 64)               65600     ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 43, 256)              657408    ['dense1[0][0]']              \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  (None, 43, 512)              1050624   ['bidirectional_4[0][0]']     \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dense2 (Dense)              (None, 43, 11)               5643      ['bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " softmax (Activation)        (None, 43, 11)               0         ['dense2[0][0]']              \n",
      "                                                                                                  \n",
      " ground_truth_labels (Input  [(None, 8)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_length (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " label_length (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " ctc (Lambda)                (None, 1)                    0         ['softmax[0][0]',             \n",
      "                                                                     'ground_truth_labels[0][0]', \n",
      "                                                                     'input_length[0][0]',        \n",
      "                                                                     'label_length[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7337035 (27.99 MB)\n",
      "Trainable params: 7332555 (27.97 MB)\n",
      "Non-trainable params: 4480 (17.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_digit_recog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>actual_digit_count</th>\n",
       "      <th>./grayscaled/</th>\n",
       "      <th>./resized/</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1500</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00000.png</td>\n",
       "      <td>./resized/00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>5743</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00001.png</td>\n",
       "      <td>./resized/00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1056</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00002.png</td>\n",
       "      <td>./resized/00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00003.png</td>\n",
       "      <td>./resized/00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>./grayscaled/00004.png</td>\n",
       "      <td>./resized/00004.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path label  \\\n",
       "0  ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...  1500   \n",
       "1  ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...  5743   \n",
       "2  ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...  1056   \n",
       "3  ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...  1000   \n",
       "4  ./ORAND-CAR-2014/CAR-A/a_test_images/a_car_007...  1000   \n",
       "\n",
       "   actual_digit_count           ./grayscaled/           ./resized/  \n",
       "0                   4  ./grayscaled/00000.png  ./resized/00000.png  \n",
       "1                   4  ./grayscaled/00001.png  ./resized/00001.png  \n",
       "2                   4  ./grayscaled/00002.png  ./resized/00002.png  \n",
       "3                   4  ./grayscaled/00003.png  ./resized/00003.png  \n",
       "4                   4  ./grayscaled/00004.png  ./resized/00004.png  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, df, img_w, img_h, batch_size, max_number_len=8):\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.batch_size = batch_size\n",
    "        self.max_number_len = max_number_len\n",
    "        \n",
    "        self.df = df\n",
    "        self.n = len(df)\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.numbers = df['label'].tolist()  # Extract related labels from DataFrame\n",
    "   \n",
    "   \n",
    "    def build_data(self):\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.df['./resized/']):\n",
    "            img = cv2.imread(img_file)\n",
    "            img = img[:,:,1]                               #Extracting Single Channel Image\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img /255\n",
    "            self.imgs[i, :, :]= img\n",
    "            if i%10000==0:\n",
    "                print(\"Loaded Images: \",i)\n",
    "           \n",
    "        print(\"Number of Texts matches with Total Number of Images :\",len(self.numbers) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "    \n",
    "    def next_data(self): \n",
    "        self.cur_index += 1\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.numbers[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n",
    "            Y_data = np.ones([self.batch_size, self.max_number_len])* -1   \n",
    "            input_length = np.ones((self.batch_size, 1)) * 40\n",
    "            label_length = np.zeros((self.batch_size, 1))                   #label length for CTC\n",
    "            source_str=[]                                                   #List to store Ground Truth Labels\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_data() #getting the image and text data pointed by current index\n",
    "                                    #taking transpose of image\n",
    "                img=img.T\n",
    "                img = np.expand_dims(img, -1)  #expanding image to have a single channel\n",
    "                X_data[i] = img\n",
    "                label=encode_numbers_labels(text) # encoding label text to integer list and storing in temp label variable\n",
    "                lbl_len=len(label)\n",
    "                Y_data[i,0:lbl_len] = label #Storing the label till its length and padding others\n",
    "                label_length[i] = len(label)\n",
    "                source_str.append(text) #storing Ground Truth Labels which will be accessed as reference for calculating metrics\n",
    "            \n",
    "        #Preparing the input for the Model\n",
    "            inputs = {\n",
    "                'img_input': X_data,  \n",
    "                'ground_truth_labels': Y_data,  \n",
    "                'input_length': input_length,  \n",
    "                'label_length': label_length,\n",
    "                'source_str': source_str  # used for visualization only\n",
    "            }\n",
    "            #Preparing output for the Model and intializing to zeros\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}  \n",
    "            yield (inputs, outputs) # Return the Prepared input and output to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  Image Loading start...\n",
      "Loaded Images:  0\n",
      "Number of Texts matches with Total Number of Images : True\n",
      "10000  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "train_gene = DataGenerator(all_labels[:10000], img_w, img_h, batch_size, max_number_len=8)\n",
    "train_gene.build_data()\n",
    "train_num_batches=int(train_gene.n / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719  Image Loading start...\n",
      "Loaded Images:  0\n",
      "Number of Texts matches with Total Number of Images : True\n",
      "1719  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "val_gen=DataGenerator(all_labels[10000:], img_w, img_h, batch_size, max_number_len=8)\n",
    "val_gen.build_data()\n",
    "val_num_batches=int(val_gen.n / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(test_func, number_batch):\n",
    "    out = test_func([number_batch])[0] #returns the predicted output matrix of the model\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = numbers_from_labels(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracies(actual_labels,predicted_labels,is_train):\n",
    "    accuracy=0\n",
    "    letter_acc=0\n",
    "    letter_cnt=0\n",
    "    count=0\n",
    "    for i in range(len(actual_labels)):\n",
    "        predicted_output=predicted_labels[i]\n",
    "        actual_output=actual_labels[i]\n",
    "        count+=1\n",
    "        for j in range(min(len(predicted_output),len(actual_output))):\n",
    "            if predicted_output[j]==actual_output[j]:\n",
    "                letter_acc+=1\n",
    "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
    "        if actual_output==predicted_output:\n",
    "            accuracy+=1\n",
    "    final_accuracy=np.round((accuracy/len(actual_labels))*100,2)\n",
    "    final_letter_acc=np.round((letter_acc/letter_cnt)*100,2)\n",
    "    return final_accuracy,final_letter_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callback visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizCallback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    The Custom Callback created for printing the Accuracy and Letter Accuracy Metrics at the End of Each Epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_func, text_img_gen,is_train,acc_compute_batches):\n",
    "        self.test_func = test_func\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.is_train=is_train                #used to indicate whether the callback is called to for Train or Validation Data\n",
    "        self.acc_batches=acc_compute_batches  # Number of Batches for which the metrics are computed typically equal to steps/epoch\n",
    "\n",
    "    def show_accuracy_metrics(self,num_batches):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy and letter accuracy for each batch of inputs, \n",
    "        and prints the avarage accuracy and letter accuracy across all the batches\n",
    "        \"\"\"\n",
    "        accuracy=0\n",
    "        letter_accuracy=0\n",
    "        batches_cnt=num_batches\n",
    "        while batches_cnt>0:\n",
    "            number_batch = next(self.text_img_gen)[0]   #Gets the next batch from the Data generator\n",
    "            decoded_res = decode_batch(self.test_func,number_batch['img_input'])\n",
    "            actual_res=number_batch['source_str']\n",
    "            acc,let_acc=accuracies(actual_res,decoded_res,self.is_train)\n",
    "            accuracy+=acc\n",
    "            letter_accuracy+=let_acc\n",
    "            batches_cnt-=1\n",
    "        accuracy=accuracy/num_batches\n",
    "        letter_accuracy=letter_accuracy/num_batches\n",
    "        if self.is_train:\n",
    "            print(\"Train Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Train Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "        else:\n",
    "            print(\"Validation Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Validation Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "            \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.show_accuracy_metrics(self.acc_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "early_stop=EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "model_chk_pt=ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False,save_weights_only=True,verbose=0, mode='auto', period=2)\n",
    "logdir = os.path.join(\"logs_127\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs_127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cb_train = VizCallback( test_func, train_gene.next_batch(),True,train_num_batches)\n",
    "viz_cb_val = VizCallback( test_func, val_gen.next_batch(),False,val_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "adam=optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_digit_recog.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajab\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\functional.py:639: UserWarning: Input dict contained keys ['source_str'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41/156 [======>.......................] - ETA: 12:28 - loss: 14.8805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m img_digit_recog\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     train_gene\u001b[39m.\u001b[39;49mnext_batch(),\n\u001b[0;32m      3\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(train_gene\u001b[39m.\u001b[39;49mn \u001b[39m/\u001b[39;49m batch_size),\n\u001b[0;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m#callbacks=[viz_cb_train, viz_cb_val, train_gene, val_gen, tensorboard_callback, early_stop, model_chk_pt],\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_gen\u001b[39m.\u001b[39;49mnext_batch(),\n\u001b[0;32m      7\u001b[0m     validation_steps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(val_gen\u001b[39m.\u001b[39;49mn \u001b[39m/\u001b[39;49m batch_size)\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = img_digit_recog.fit(\n",
    "    train_gene.next_batch(),\n",
    "    steps_per_epoch=int(train_gene.n / batch_size),\n",
    "    epochs=20,\n",
    "    #callbacks=[viz_cb_train, viz_cb_val, train_gene, val_gen, tensorboard_callback, early_stop, model_chk_pt],\n",
    "    validation_data=val_gen.next_batch(),\n",
    "    validation_steps=int(val_gen.n / batch_size)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
